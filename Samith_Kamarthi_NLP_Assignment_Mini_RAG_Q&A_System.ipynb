{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install faiss-cpu sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91ZxTRboACoh",
        "outputId": "685c49d7-bc34-4495-ef49-7cdfee77899a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.57.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.11.12)\n",
            "Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import google.generativeai as genai\n",
        "\n",
        "# ------------------------ CONFIGURATION ------------------------\n",
        "# Load API key from environment variable\n",
        "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "# If key is not found in environment, prompt user to enter it\n",
        "if not API_KEY:\n",
        "    print(\"‚ö†Ô∏è  GEMINI_API_KEY not found in environment variables.\")\n",
        "    user_key = input(\"üîë Please enter your Google AI Studio API Key (or press Enter to skip): \").strip()\n",
        "    if user_key:\n",
        "        API_KEY = user_key\n",
        "    else:\n",
        "        API_KEY = \"YOUR_API_KEY_HERE\"\n",
        "\n",
        "# Configure Gemini API if key is valid\n",
        "if API_KEY == \"YOUR_API_KEY_HERE\" or API_KEY is None:\n",
        "    print(\"‚ö†Ô∏è  WARNING: API_KEY not provided.\")\n",
        "    print(\"   The retrieval part will work, but generation and scoring will be skipped.\\n\")\n",
        "else:\n",
        "    genai.configure(api_key=API_KEY)\n",
        "\n",
        "# ------------------------ KNOWLEDGE BASE ------------------------\n",
        "# Example text passages about the Solar System\n",
        "knowledge_base = [\n",
        "    \"The Sun is the star at the center of the Solar System. It is a nearly perfect sphere of hot plasma.\",\n",
        "    \"Mercury is the smallest planet in the Solar System and the closest to the Sun. It takes 88 Earth days to orbit the Sun.\",\n",
        "    \"Venus is the second planet from the Sun. It has a thick atmosphere that traps heat, making it the hottest planet.\",\n",
        "    \"Earth is the third planet from the Sun and the only astronomical object known to harbor life.\",\n",
        "    \"Mars is the fourth planet from the Sun and is often called the 'Red Planet' due to reddish iron oxide on its surface.\",\n",
        "    \"Jupiter is the largest planet in the Solar System. It is a gas giant with a mass more than two and a half times that of all other planets combined.\",\n",
        "    \"Saturn is the sixth planet from the Sun and is famous for its prominent ring system, which is composed mainly of ice particles.\",\n",
        "    \"Uranus is the seventh planet from the Sun. It is a unique tilt, rotating on its side compared to the plane of the Solar System.\",\n",
        "    \"Neptune is the eighth and farthest-known Solar planet from the Sun. It is a dense, giant planet known for its strong winds.\",\n",
        "    \"Pluto, once considered the ninth planet, was reclassified as a dwarf planet in 2006 by the IAU.\",\n",
        "    \"The asteroid belt is a torus-shaped region in the Solar System, located roughly between the orbits of Mars and Jupiter.\",\n",
        "    \"Comets are cosmic snowballs of frozen gases, rock, and dust that orbit the Sun. When frozen, they are the size of a small town.\"\n",
        "]\n",
        "\n",
        "print(f\"üìö Loaded {len(knowledge_base)} text passages into the Knowledge Base.\")\n",
        "\n",
        "# ------------------------ EMBEDDINGS ------------------------\n",
        "# Load Sentence Transformer model for encoding text into embeddings\n",
        "print(\"üîÑ Loading Sentence Transformer model (this may take a moment)...\")\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Generate embeddings for all knowledge base passages\n",
        "print(\"‚ö° Generating embeddings for knowledge base...\")\n",
        "corpus_embeddings = embedder.encode(knowledge_base)\n",
        "print(f\"   Embeddings shape: {corpus_embeddings.shape}\")\n",
        "\n",
        "# ------------------------ FAISS INDEX ------------------------\n",
        "# Create FAISS index for similarity search using L2 (Euclidean) distance\n",
        "d = corpus_embeddings.shape[1]  # dimension of embeddings\n",
        "index = faiss.IndexFlatL2(d)\n",
        "index.add(corpus_embeddings)\n",
        "print(f\"üóÇÔ∏è  FAISS Index built with {index.ntotal} vectors.\\n\")\n",
        "\n",
        "# ------------------------ RETRIEVAL FUNCTION ------------------------\n",
        "def retrieve_context(query, k=3):\n",
        "    \"\"\"\n",
        "    Retrieve top-k most relevant knowledge chunks for a given query.\n",
        "    \"\"\"\n",
        "    query_embedding = embedder.encode([query])\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "    retrieved_chunks = [knowledge_base[idx] for idx in indices[0]]\n",
        "    return retrieved_chunks\n",
        "\n",
        "# ------------------------ GENERATION FUNCTION ------------------------\n",
        "def generate_answer(query, context_chunks):\n",
        "    \"\"\"\n",
        "    Use Gemini LLM to generate an answer using only the provided context.\n",
        "    \"\"\"\n",
        "    if not API_KEY or API_KEY == \"YOUR_API_KEY_HERE\":\n",
        "        return \"Skipped (No API Key)\"\n",
        "\n",
        "    context_str = \"\\n\".join([f\"- {chunk}\" for chunk in context_chunks])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a helpful assistant. Answer the user's question using ONLY the context provided below.\n",
        "    If the answer is not in the context, say \"I don't know based on the provided text.\"\n",
        "\n",
        "    Context:\n",
        "    {context_str}\n",
        "\n",
        "    Question: {query}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text.strip()\n",
        "\n",
        "# ------------------------ EVALUATION FUNCTION ------------------------\n",
        "def evaluate_rag(query, context_chunks, answer):\n",
        "    \"\"\"\n",
        "    Evaluate the faithfulness and relevance of the generated answer to the retrieved context.\n",
        "    \"\"\"\n",
        "    if not API_KEY or API_KEY == \"YOUR_API_KEY_HERE\":\n",
        "        return \"Skipped (No API Key)\"\n",
        "\n",
        "    context_str = \"\\n\".join([f\"- {chunk}\" for chunk in context_chunks])\n",
        "\n",
        "    eval_prompt = f\"\"\"\n",
        "    You are a strict judge. Evaluate the quality of this RAG (Retrieval-Augmented Generation) interaction.\n",
        "\n",
        "    1. User Question: {query}\n",
        "    2. Retrieved Context: {context_str}\n",
        "    3. AI Answer: {answer}\n",
        "\n",
        "    Scoring Criteria:\n",
        "    - **Context Relevance**: Does the retrieved context actually contain information relevant to the User Question? If the question is unrelated to the context, give a low score (1-2).\n",
        "    - **Faithfulness**: Did the AI answer using ONLY the provided context?\n",
        "\n",
        "    Score Guide:\n",
        "    1: Context is irrelevant to the question OR Answer hallucinates/ignores context.\n",
        "    3: Context is partially relevant.\n",
        "    5: Context is perfectly relevant AND Answer is accurate based on context.\n",
        "\n",
        "    Output Format:\n",
        "    Score: [1-5]\n",
        "    Explanation: [Reasoning]\n",
        "    \"\"\"\n",
        "\n",
        "    model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "    response = model.generate_content(eval_prompt)\n",
        "    return response.text.strip()\n",
        "\n",
        "# ------------------------ MAIN EXECUTION LOOP ------------------------\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Interactive Mini-RAG system loop:\n",
        "    1. Accept user query\n",
        "    2. Retrieve relevant chunks\n",
        "    3. Generate answer with LLM\n",
        "    4. Optionally evaluate answer\n",
        "    \"\"\"\n",
        "    print(\"--- üöÄ Mini-RAG System Ready ---\")\n",
        "    print(\"Type 'exit' to quit.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"‚ùì Enter your question: \")\n",
        "        if user_query.lower() in ['exit', 'quit']:\n",
        "            break\n",
        "\n",
        "        print(\"   üîé Retrieving relevant chunks...\")\n",
        "        relevant_docs = retrieve_context(user_query, k=2)\n",
        "\n",
        "        print(\"\\n   üìÑ Retrieved Context:\")\n",
        "        for i, doc in enumerate(relevant_docs):\n",
        "            print(f\"      {i+1}. {doc}\")\n",
        "\n",
        "        print(\"\\n   ü§ñ Generating Answer...\")\n",
        "        answer = generate_answer(user_query, relevant_docs)\n",
        "        print(f\"   >> Answer: {answer}\")\n",
        "\n",
        "        if API_KEY and API_KEY != \"YOUR_API_KEY_HERE\":\n",
        "            print(\"\\n   ‚öñÔ∏è  Evaluating Response...\")\n",
        "            score = evaluate_rag(user_query, relevant_docs, answer)\n",
        "            print(f\"   >> Evaluation:\\n{score}\")\n",
        "\n",
        "        print(\"-\" * 50 + \"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "id": "udaHMA8vTnAw",
        "outputId": "d86ce657-fba4-4ced-9ed0-1f2cf8fa4826"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è  GEMINI_API_KEY not found in environment variables.\n",
            "üîë Please enter your Google AI Studio API Key (or press Enter to skip): AIzaSyCGJkvRjKSpLwNS_7K_jf8nJwmff4_2diI\n",
            "üìö Loaded 12 text passages into the Knowledge Base.\n",
            "üîÑ Loading Sentence Transformer model (this may take a moment)...\n",
            "‚ö° Generating embeddings for knowledge base...\n",
            "   Embeddings shape: (12, 384)\n",
            "üóÇÔ∏è  FAISS Index built with 12 vectors.\n",
            "\n",
            "--- üöÄ Mini-RAG System Ready ---\n",
            "Type 'exit' to quit.\n",
            "\n",
            "‚ùì Enter your question: best car in USA\n",
            "   üîé Retrieving relevant chunks...\n",
            "\n",
            "   üìÑ Retrieved Context:\n",
            "      1. Venus is the second planet from the Sun. It has a thick atmosphere that traps heat, making it the hottest planet.\n",
            "      2. Saturn is the sixth planet from the Sun and is famous for its prominent ring system, which is composed mainly of ice particles.\n",
            "\n",
            "   ü§ñ Generating Answer...\n",
            "   >> Answer: I don't know based on the provided text.\n",
            "\n",
            "   ‚öñÔ∏è  Evaluating Response...\n",
            "   >> Evaluation:\n",
            "Score: 1\n",
            "Explanation: The retrieved context is entirely irrelevant to the user's question about cars, as it discusses planets (Venus and Saturn). While the AI's answer correctly states it cannot answer based on the provided text, the fundamental failure lies in the retrieval step providing completely unrelated information. According to the scoring criteria, \"Context is irrelevant to the question\" warrants a score of 1.\n",
            "--------------------------------------------------\n",
            "\n",
            "‚ùì Enter your question: which is the largest planet \n",
            "   üîé Retrieving relevant chunks...\n",
            "\n",
            "   üìÑ Retrieved Context:\n",
            "      1. Jupiter is the largest planet in the Solar System. It is a gas giant with a mass more than two and a half times that of all other planets combined.\n",
            "      2. Neptune is the eighth and farthest-known Solar planet from the Sun. It is a dense, giant planet known for its strong winds.\n",
            "\n",
            "   ü§ñ Generating Answer...\n",
            "   >> Answer: Jupiter is the largest planet in the Solar System.\n",
            "\n",
            "   ‚öñÔ∏è  Evaluating Response...\n",
            "   >> Evaluation:\n",
            "Score: 5\n",
            "Explanation: The retrieved context directly and explicitly answers the user's question about the largest planet. The AI's answer faithfully extracts this information solely from the provided context, without any additions or omissions that would alter its accuracy or faithfulness.\n",
            "--------------------------------------------------\n",
            "\n",
            "‚ùì Enter your question: exit\n"
          ]
        }
      ]
    }
  ]
}