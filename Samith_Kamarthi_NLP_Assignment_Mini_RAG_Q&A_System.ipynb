{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install faiss-cpu sentence_transformers"
      ],
      "metadata": {
        "id": "91ZxTRboACoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import google.generativeai as genai\n",
        "\n",
        "# ------------------------ CONFIGURATION ------------------------\n",
        "# Load API key from environment variable\n",
        "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "# If key is not found in environment, prompt user to enter it\n",
        "if not API_KEY:\n",
        "    print(\"âš ï¸  GEMINI_API_KEY not found in environment variables.\")\n",
        "    user_key = input(\"ðŸ”‘ Please enter your Google AI Studio API Key (or press Enter to skip): \").strip()\n",
        "    if user_key:\n",
        "        API_KEY = user_key\n",
        "    else:\n",
        "        API_KEY = \"YOUR_API_KEY_HERE\"\n",
        "\n",
        "# Configure Gemini API if key is valid\n",
        "if API_KEY == \"YOUR_API_KEY_HERE\" or API_KEY is None:\n",
        "    print(\"âš ï¸  WARNING: API_KEY not provided.\")\n",
        "    print(\"   The retrieval part will work, but generation and scoring will be skipped.\\n\")\n",
        "else:\n",
        "    genai.configure(api_key=API_KEY)\n",
        "\n",
        "# ------------------------ KNOWLEDGE BASE ------------------------\n",
        "# Example text passages about the Solar System\n",
        "knowledge_base = [\n",
        "    \"The Sun is the star at the center of the Solar System. It is a nearly perfect sphere of hot plasma.\",\n",
        "    \"Mercury is the smallest planet in the Solar System and the closest to the Sun. It takes 88 Earth days to orbit the Sun.\",\n",
        "    \"Venus is the second planet from the Sun. It has a thick atmosphere that traps heat, making it the hottest planet.\",\n",
        "    \"Earth is the third planet from the Sun and the only astronomical object known to harbor life.\",\n",
        "    \"Mars is the fourth planet from the Sun and is often called the 'Red Planet' due to reddish iron oxide on its surface.\",\n",
        "    \"Jupiter is the largest planet in the Solar System. It is a gas giant with a mass more than two and a half times that of all other planets combined.\",\n",
        "    \"Saturn is the sixth planet from the Sun and is famous for its prominent ring system, which is composed mainly of ice particles.\",\n",
        "    \"Uranus is the seventh planet from the Sun. It is a unique tilt, rotating on its side compared to the plane of the Solar System.\",\n",
        "    \"Neptune is the eighth and farthest-known Solar planet from the Sun. It is a dense, giant planet known for its strong winds.\",\n",
        "    \"Pluto, once considered the ninth planet, was reclassified as a dwarf planet in 2006 by the IAU.\",\n",
        "    \"The asteroid belt is a torus-shaped region in the Solar System, located roughly between the orbits of Mars and Jupiter.\",\n",
        "    \"Comets are cosmic snowballs of frozen gases, rock, and dust that orbit the Sun. When frozen, they are the size of a small town.\"\n",
        "]\n",
        "\n",
        "print(f\"ðŸ“š Loaded {len(knowledge_base)} text passages into the Knowledge Base.\")\n",
        "\n",
        "# ------------------------ EMBEDDINGS ------------------------\n",
        "# Load Sentence Transformer model for encoding text into embeddings\n",
        "print(\"ðŸ”„ Loading Sentence Transformer model (this may take a moment)...\")\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Generate embeddings for all knowledge base passages\n",
        "print(\"âš¡ Generating embeddings for knowledge base...\")\n",
        "corpus_embeddings = embedder.encode(knowledge_base)\n",
        "print(f\"   Embeddings shape: {corpus_embeddings.shape}\")\n",
        "\n",
        "# ------------------------ FAISS INDEX ------------------------\n",
        "# Create FAISS index for similarity search using L2 (Euclidean) distance\n",
        "d = corpus_embeddings.shape[1]  # dimension of embeddings\n",
        "index = faiss.IndexFlatL2(d)\n",
        "index.add(corpus_embeddings)\n",
        "print(f\"ðŸ—‚ï¸  FAISS Index built with {index.ntotal} vectors.\\n\")\n",
        "\n",
        "# ------------------------ RETRIEVAL FUNCTION ------------------------\n",
        "def retrieve_context(query, k=3):\n",
        "    \"\"\"\n",
        "    Retrieve top-k most relevant knowledge chunks for a given query.\n",
        "    \"\"\"\n",
        "    query_embedding = embedder.encode([query])\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "    retrieved_chunks = [knowledge_base[idx] for idx in indices[0]]\n",
        "    return retrieved_chunks\n",
        "\n",
        "# ------------------------ GENERATION FUNCTION ------------------------\n",
        "def generate_answer(query, context_chunks):\n",
        "    \"\"\"\n",
        "    Use Gemini LLM to generate an answer using only the provided context.\n",
        "    \"\"\"\n",
        "    if not API_KEY or API_KEY == \"YOUR_API_KEY_HERE\":\n",
        "        return \"Skipped (No API Key)\"\n",
        "\n",
        "    context_str = \"\\n\".join([f\"- {chunk}\" for chunk in context_chunks])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a helpful assistant. Answer the user's question using ONLY the context provided below.\n",
        "    If the answer is not in the context, say \"I don't know based on the provided text.\"\n",
        "\n",
        "    Context:\n",
        "    {context_str}\n",
        "\n",
        "    Question: {query}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text.strip()\n",
        "\n",
        "# ------------------------ EVALUATION FUNCTION ------------------------\n",
        "def evaluate_rag(query, context_chunks, answer):\n",
        "    \"\"\"\n",
        "    Evaluate the faithfulness and relevance of the generated answer to the retrieved context.\n",
        "    \"\"\"\n",
        "    if not API_KEY or API_KEY == \"YOUR_API_KEY_HERE\":\n",
        "        return \"Skipped (No API Key)\"\n",
        "\n",
        "    context_str = \"\\n\".join([f\"- {chunk}\" for chunk in context_chunks])\n",
        "\n",
        "    eval_prompt = f\"\"\"\n",
        "    You are a strict judge. Evaluate the quality of this RAG (Retrieval-Augmented Generation) interaction.\n",
        "\n",
        "    1. User Question: {query}\n",
        "    2. Retrieved Context: {context_str}\n",
        "    3. AI Answer: {answer}\n",
        "\n",
        "    Scoring Criteria:\n",
        "    - **Context Relevance**: Does the retrieved context actually contain information relevant to the User Question? If the question is unrelated to the context, give a low score (1-2).\n",
        "    - **Faithfulness**: Did the AI answer using ONLY the provided context?\n",
        "\n",
        "    Score Guide:\n",
        "    1: Context is irrelevant to the question OR Answer hallucinates/ignores context.\n",
        "    3: Context is partially relevant.\n",
        "    5: Context is perfectly relevant AND Answer is accurate based on context.\n",
        "\n",
        "    Output Format:\n",
        "    Score: [1-5]\n",
        "    Explanation: [Reasoning]\n",
        "    \"\"\"\n",
        "\n",
        "    model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "    response = model.generate_content(eval_prompt)\n",
        "    return response.text.strip()\n",
        "\n",
        "# ------------------------ MAIN EXECUTION LOOP ------------------------\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Interactive Mini-RAG system loop:\n",
        "    1. Accept user query\n",
        "    2. Retrieve relevant chunks\n",
        "    3. Generate answer with LLM\n",
        "    4. Optionally evaluate answer\n",
        "    \"\"\"\n",
        "    print(\"--- ðŸš€ Mini-RAG System Ready ---\")\n",
        "    print(\"Type 'exit' to quit.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"â“ Enter your question: \")\n",
        "        if user_query.lower() in ['exit', 'quit']:\n",
        "            break\n",
        "\n",
        "        print(\"   ðŸ”Ž Retrieving relevant chunks...\")\n",
        "        relevant_docs = retrieve_context(user_query, k=2)\n",
        "\n",
        "        print(\"\\n   ðŸ“„ Retrieved Context:\")\n",
        "        for i, doc in enumerate(relevant_docs):\n",
        "            print(f\"      {i+1}. {doc}\")\n",
        "\n",
        "        print(\"\\n   ðŸ¤– Generating Answer...\")\n",
        "        answer = generate_answer(user_query, relevant_docs)\n",
        "        print(f\"   >> Answer: {answer}\")\n",
        "\n",
        "        if API_KEY and API_KEY != \"YOUR_API_KEY_HERE\":\n",
        "            print(\"\\n   âš–ï¸  Evaluating Response...\")\n",
        "            score = evaluate_rag(user_query, relevant_docs, answer)\n",
        "            print(f\"   >> Evaluation:\\n{score}\")\n",
        "\n",
        "        print(\"-\" * 50 + \"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "udaHMA8vTnAw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}